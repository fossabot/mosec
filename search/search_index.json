{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Model Serving made Efficient in the Cloud. Introduction \u00a4 Mosec is a high-performance and flexible model serving framework for building ML model-enabled backends and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. Highly performant : web layer and task coordination built with Rust \ud83e\udd80, which offers blazing speed in addition to efficient CPU utilization powered by async I/O Ease of use : user interface purely in Python \ud83d\udc0d, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing Dynamic batching : aggregate requests from different users for batched inference and distribute results back Pipelined stages : spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads Installation \u00a4 Mosec requires Python 3.6 or above. Install the latest PyPI package with: pip install -U mosec Usage \u00a4 Write the server \u00a4 Import the libraries and setup a basic logger to better observe what happens: import logging from pydantic import BaseModel # we need this to define our input/output schemas from mosec import Server , Worker logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) Define our service schemas for both input and output. These schemas will help us for data validation: class Request ( BaseModel ): x : float class Response ( BaseModel ): y : float Now, we are going to build an API to calculate the exponential with base e for a given number. To achieve that, we simply inherit the Worker class and override the forward method: import math class CalculateExp ( Worker ): def forward ( self , req : Request ): y = math . exp ( req . x ) # f(x) = e ^ x logger . debug ( f \"e ^ { req . x } = { y } \" ) return Response ( y = y ) Finally, we run the server when the file is executed: if __name__ == \"__main__\" : server = Server ( Request , Response ) server . append_worker ( CalculateExp , num = 2 ) # we spawn two processes for our calculator server . run () Run the server \u00a4 After merging the snippets above into a file named server.py , we can first have a look at the supported arguments: python server.py --help Then let's start the server... python server.py and test it: curl -X POST http://127.0.0.1:8000/inference -d '{\"x\": 2}' That's it! You have just hosted your exponential-computing model as a server! \ud83d\ude09 Example \u00a4 More ready-to-use examples can be found in the Example section. Contributing \u00a4 We welcome any kind of contributions. Please give us feedback by raising issues or directly contribute your code and pull request!","title":"Overview"},{"location":"#introduction","text":"Mosec is a high-performance and flexible model serving framework for building ML model-enabled backends and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. Highly performant : web layer and task coordination built with Rust \ud83e\udd80, which offers blazing speed in addition to efficient CPU utilization powered by async I/O Ease of use : user interface purely in Python \ud83d\udc0d, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing Dynamic batching : aggregate requests from different users for batched inference and distribute results back Pipelined stages : spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads","title":"Introduction"},{"location":"#installation","text":"Mosec requires Python 3.6 or above. Install the latest PyPI package with: pip install -U mosec","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#write-the-server","text":"Import the libraries and setup a basic logger to better observe what happens: import logging from pydantic import BaseModel # we need this to define our input/output schemas from mosec import Server , Worker logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) Define our service schemas for both input and output. These schemas will help us for data validation: class Request ( BaseModel ): x : float class Response ( BaseModel ): y : float Now, we are going to build an API to calculate the exponential with base e for a given number. To achieve that, we simply inherit the Worker class and override the forward method: import math class CalculateExp ( Worker ): def forward ( self , req : Request ): y = math . exp ( req . x ) # f(x) = e ^ x logger . debug ( f \"e ^ { req . x } = { y } \" ) return Response ( y = y ) Finally, we run the server when the file is executed: if __name__ == \"__main__\" : server = Server ( Request , Response ) server . append_worker ( CalculateExp , num = 2 ) # we spawn two processes for our calculator server . run ()","title":"Write the server"},{"location":"#run-the-server","text":"After merging the snippets above into a file named server.py , we can first have a look at the supported arguments: python server.py --help Then let's start the server... python server.py and test it: curl -X POST http://127.0.0.1:8000/inference -d '{\"x\": 2}' That's it! You have just hosted your exponential-computing model as a server! \ud83d\ude09","title":"Run the server"},{"location":"#example","text":"More ready-to-use examples can be found in the Example section.","title":"Example"},{"location":"#contributing","text":"We welcome any kind of contributions. Please give us feedback by raising issues or directly contribute your code and pull request!","title":"Contributing"},{"location":"argument/","text":"usage: your_model_server.py [-h] [--path PATH] [--capacity CAPACITY] [--timeout TIMEOUT] [--wait WAIT] [--address ADDRESS] [--port PORT] [--namespace NAMESPACE] Mosec Server Configurations optional arguments: -h, --help show this help message and exit --path PATH Unix Domain Socket address for internal Inter-Process Communication --capacity CAPACITY Capacity of the request queue, beyond which new requests will be rejected with status 429 --timeout TIMEOUT Service timeout for one request (milliseconds) --wait WAIT Wait time for the batcher to batch (milliseconds) --address ADDRESS Address of the HTTP service --port PORT Port of the HTTP service --namespace NAMESPACE Namespace for prometheus metrics","title":"Argument"},{"location":"contributing/","text":"Before contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Pull Request Process \u00a4 After you have forked this repository, you could use make install for the first time to install the local development dependencies; afterwards, you may use make dev to build the library when you have made any code changes. Before committing your changes, you can use make format followed by make lint to ensure the codes are aligned with our style standards. Increase the version number (discussed with the repository owner) and update docs or README.md when necessary. The versioning scheme we use is SemVer . Submit your pull request. Contacts \u00a4 Keming kemingy94@gmail.com zclzc lkevinzc@gmail.com","title":"Contributing"},{"location":"contributing/#pull-request-process","text":"After you have forked this repository, you could use make install for the first time to install the local development dependencies; afterwards, you may use make dev to build the library when you have made any code changes. Before committing your changes, you can use make format followed by make lint to ensure the codes are aligned with our style standards. Increase the version number (discussed with the repository owner) and update docs or README.md when necessary. The versioning scheme we use is SemVer . Submit your pull request.","title":"Pull Request Process"},{"location":"contributing/#contacts","text":"Keming kemingy94@gmail.com zclzc lkevinzc@gmail.com","title":"Contacts"},{"location":"interface/","text":"mosec.worker \u00a4 mosec.worker.Worker \u00a4 This public class defines the mosec worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the forward method to be implemented by the users. In addition, users may also want to override the deserialize method in the first stage (we term it as ingress stage)) together with the serialize method in the last stage (we term it as egress stage). By default, we use JSON protocol and utilize pydantic for schema validation. Users are free to choose MessagePack , BSON and other protocols alternatively. Note \u00a4 The \" same type \" mentioned below is applicable only when the stage disables batching. For a stage that enables batching , the worker 's forward should accept a list and output a list, where each element will follow the \" same type \" constraint. mosec . worker . Worker . deserialize ( self , data : bytes ) -> Any \u00a4 This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data bytes the raw bytes extracted from the request body required Returns: Type Description Any the *same type as the argument of the forward you implement Source code in mosec/worker.py def deserialize ( self , data : bytes ) -> Any : \"\"\" This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement \"\"\" return json . loads ( data ) if data else {} mosec . worker . Worker . forward ( self , data : Any ) -> Any \u00a4 This method defines the worker's main logic, be it data processing, computation or model inference. Must be overridden by the subclass. The implementation should make sure: (for a single-stage worker) both the input and output follow the *same type rule. (for a multi-stage worker) the input of the ingress stage and the output of the egress stage follow the *same type , while others should align with its adjacent stage's in/output. If any code in the forward needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the __init__ method. Source code in mosec/worker.py def forward ( self , data : Any ) -> Any : \"\"\" This method defines the worker's main logic, be it data processing, computation or model inference. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError mosec . worker . Worker . serialize ( self , data : Any ) -> bytes \u00a4 This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data Any the *same type as the output of the forward you implement required Returns: Type Description bytes the bytes you want to put into the response body Source code in mosec/worker.py def serialize ( self , data : Any ) -> bytes : \"\"\" This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body \"\"\" return json . dumps ( data , indent = 2 , default = pydantic_encoder ) . encode () mosec.server \u00a4 mosec.server.Server \u00a4 This public class defines the mosec server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server. Batching \u00a4 The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the max_batch_size . Multiprocess \u00a4 The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the num . mosec . server . Server . append_worker ( self , worker : Type [ mosec . worker . Worker ], num : ConstrainedIntValue = 1 , max_batch_size : ConstrainedIntValue = 1 , start_method : str = 'spawn' ) \u00a4 This method sequentially appends workers to the workflow pipeline. Parameters: Name Type Description Default worker Type[mosec.worker.Worker] the class you inherit from Worker which implements the forward method required num ConstrainedIntValue the number of processes for parallel computing (>=1) 1 max_batch_size ConstrainedIntValue the maximum batch size allowed (>=1) 1 start_method str the process starting method (\"spawn\" or \"fork\") 'spawn' Source code in mosec/server.py @validate_arguments def append_worker ( self , worker : Type [ Worker ], num : AtLeastOne = 1 , # type: ignore max_batch_size : AtLeastOne = 1 , # type: ignore start_method : str = \"spawn\" , ): \"\"\" This method sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") \"\"\" assert ( start_method in NEW_PROCESS_METHOD ), f \"start method needs to be one of { NEW_PROCESS_METHOD } \" self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num ) mosec . server . Server . run ( self ) \u00a4 This method starts the mosec model server! Source code in mosec/server.py def run ( self ): \"\"\" This method starts the mosec model server! \"\"\" self . _validate () self . _parse_args () self . _start_controller () try : self . _manage_coordinators () except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt ()","title":"Interface"},{"location":"interface/#mosec.worker","text":"","title":"worker"},{"location":"interface/#mosec.worker.Worker","text":"This public class defines the mosec worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the forward method to be implemented by the users. In addition, users may also want to override the deserialize method in the first stage (we term it as ingress stage)) together with the serialize method in the last stage (we term it as egress stage). By default, we use JSON protocol and utilize pydantic for schema validation. Users are free to choose MessagePack , BSON and other protocols alternatively.","title":"Worker"},{"location":"interface/#mosec.worker.Worker--note","text":"The \" same type \" mentioned below is applicable only when the stage disables batching. For a stage that enables batching , the worker 's forward should accept a list and output a list, where each element will follow the \" same type \" constraint.","title":"Note"},{"location":"interface/#mosec.worker.Worker.deserialize","text":"This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data bytes the raw bytes extracted from the request body required Returns: Type Description Any the *same type as the argument of the forward you implement Source code in mosec/worker.py def deserialize ( self , data : bytes ) -> Any : \"\"\" This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement \"\"\" return json . loads ( data ) if data else {}","title":"deserialize()"},{"location":"interface/#mosec.worker.Worker.forward","text":"This method defines the worker's main logic, be it data processing, computation or model inference. Must be overridden by the subclass. The implementation should make sure: (for a single-stage worker) both the input and output follow the *same type rule. (for a multi-stage worker) the input of the ingress stage and the output of the egress stage follow the *same type , while others should align with its adjacent stage's in/output. If any code in the forward needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the __init__ method. Source code in mosec/worker.py def forward ( self , data : Any ) -> Any : \"\"\" This method defines the worker's main logic, be it data processing, computation or model inference. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"interface/#mosec.worker.Worker.serialize","text":"This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data Any the *same type as the output of the forward you implement required Returns: Type Description bytes the bytes you want to put into the response body Source code in mosec/worker.py def serialize ( self , data : Any ) -> bytes : \"\"\" This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body \"\"\" return json . dumps ( data , indent = 2 , default = pydantic_encoder ) . encode ()","title":"serialize()"},{"location":"interface/#mosec.server","text":"","title":"server"},{"location":"interface/#mosec.server.Server","text":"This public class defines the mosec server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server.","title":"Server"},{"location":"interface/#mosec.server.Server--batching","text":"The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the max_batch_size .","title":"Batching"},{"location":"interface/#mosec.server.Server--multiprocess","text":"The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the num .","title":"Multiprocess"},{"location":"interface/#mosec.server.Server.append_worker","text":"This method sequentially appends workers to the workflow pipeline. Parameters: Name Type Description Default worker Type[mosec.worker.Worker] the class you inherit from Worker which implements the forward method required num ConstrainedIntValue the number of processes for parallel computing (>=1) 1 max_batch_size ConstrainedIntValue the maximum batch size allowed (>=1) 1 start_method str the process starting method (\"spawn\" or \"fork\") 'spawn' Source code in mosec/server.py @validate_arguments def append_worker ( self , worker : Type [ Worker ], num : AtLeastOne = 1 , # type: ignore max_batch_size : AtLeastOne = 1 , # type: ignore start_method : str = \"spawn\" , ): \"\"\" This method sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") \"\"\" assert ( start_method in NEW_PROCESS_METHOD ), f \"start method needs to be one of { NEW_PROCESS_METHOD } \" self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num )","title":"append_worker()"},{"location":"interface/#mosec.server.Server.run","text":"This method starts the mosec model server! Source code in mosec/server.py def run ( self ): \"\"\" This method starts the mosec model server! \"\"\" self . _validate () self . _parse_args () self . _start_controller () try : self . _manage_coordinators () except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt ()","title":"run()"},{"location":"example/","text":"We provide examples across different ML frameworks and for various tasks in this section. Get started \u00a4 All the examples in this section are self-contained and tested, feel free to grab one and run: python model_server.py To test the server, we use httpie and httpx by default. You can have other choices but if you want to install them: pip install httpie httpx","title":"Overview"},{"location":"example/#get-started","text":"All the examples in this section are self-contained and tested, feel free to grab one and run: python model_server.py To test the server, we use httpie and httpx by default. You can have other choices but if you want to install them: pip install httpie httpx","title":"Get started"},{"location":"example/echo/","text":"An echo server is usually the very first server you wanna implement to get familiar with the framework. This server sleeps for a given period of time and return. It is a simple illustration of how multi-stage workload is implemented. echo.py \u00a4 import logging import time from typing import List from pydantic import BaseModel from mosec import Server , Worker logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class EchoReq ( BaseModel ): time : float class EchoResp ( BaseModel ): msg : str class Preprocess ( Worker ): def forward ( self , data : EchoReq ) -> float : logger . debug ( f \"pre received { data } \" ) return data . time class Inference ( Worker ): def forward ( self , data : List [ float ]) -> List [ float ]: logger . info ( f \"received batch size: { len ( data ) } \" ) time . sleep ( sum ( data ) / len ( data )) return data class Postprocess ( Worker ): def forward ( self , data : float ) -> EchoResp : logger . debug ( f \"post received { data } \" ) return EchoResp ( msg = f \"sleep { data } seconds\" ) if __name__ == \"__main__\" : server = Server ( EchoReq , EchoResp ) server . append_worker ( Preprocess , num = 2 ) server . append_worker ( Inference , max_batch_size = 16 ) server . append_worker ( Postprocess , num = 2 ) server . run () Start \u00a4 python echo.py Test \u00a4 http :8000/inference time=1.5","title":"Echo"},{"location":"example/echo/#echopy","text":"import logging import time from typing import List from pydantic import BaseModel from mosec import Server , Worker logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class EchoReq ( BaseModel ): time : float class EchoResp ( BaseModel ): msg : str class Preprocess ( Worker ): def forward ( self , data : EchoReq ) -> float : logger . debug ( f \"pre received { data } \" ) return data . time class Inference ( Worker ): def forward ( self , data : List [ float ]) -> List [ float ]: logger . info ( f \"received batch size: { len ( data ) } \" ) time . sleep ( sum ( data ) / len ( data )) return data class Postprocess ( Worker ): def forward ( self , data : float ) -> EchoResp : logger . debug ( f \"post received { data } \" ) return EchoResp ( msg = f \"sleep { data } seconds\" ) if __name__ == \"__main__\" : server = Server ( EchoReq , EchoResp ) server . append_worker ( Preprocess , num = 2 ) server . append_worker ( Inference , max_batch_size = 16 ) server . append_worker ( Postprocess , num = 2 ) server . run ()","title":"echo.py"},{"location":"example/echo/#start","text":"python echo.py","title":"Start"},{"location":"example/echo/#test","text":"http :8000/inference time=1.5","title":"Test"},{"location":"example/pytorch/","text":"Here are some out-of-the-box model servers powered by mosec for PyTorch users. We use the version 1.9.0 in the following examples. Computer Vision \u00a4 Computer vision model servers usually receive images or links to the images (downloading from the link becomes an I/O workload then), feed the preprocessed image data into the model and extract information from the output results. Image Recognition \u00a4 This server receives an image and classify it according to the ImageNet categorization. We specifically use ResNet as an image classifier and build a model service based on it. Nevertheless, this file serves as the starter code for any kind of image recognition model server. We enable multiprocessing for Preprocess stage, so that it can produce enough tasks for Inference stage to do batch inference , which better exploits the GPU computing power. resnet50.py import base64 import logging from typing import List from urllib.request import urlretrieve import cv2 # type: ignore import numpy as np # type: ignore import torch # type: ignore import torchvision # type: ignore from pydantic import BaseModel from mosec import Server , Worker logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 16 class ImageReq ( BaseModel ): image : str # base64 encoded class CategoryResp ( BaseModel ): category : str # class name class Preprocess ( Worker ): def forward ( self , req : ImageReq ) -> np . ndarray : im = np . frombuffer ( base64 . b64decode ( req . image ), np . uint8 ) im = cv2 . imdecode ( im , cv2 . IMREAD_COLOR )[:, :, :: - 1 ] # bgr -> rgb im = cv2 . resize ( im , ( 256 , 256 )) crop_im = ( im [ 16 : 16 + 224 , 16 : 16 + 224 ] . astype ( np . float32 ) / 255 ) # center crop crop_im -= [ 0.485 , 0.456 , 0.406 ] crop_im /= [ 0.229 , 0.224 , 0.225 ] crop_im = np . transpose ( crop_im , ( 2 , 0 , 1 )) return crop_im class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = torchvision . models . resnet50 ( pretrained = True ) self . model . eval () self . model . to ( self . device ) # overwrite self.example for warmup self . example = [ np . zeros (( 3 , 244 , 244 ), dtype = np . float32 ) ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ np . ndarray ]) -> List [ int ]: logger . info ( f \"processing batch with size: { len ( data ) } \" ) with torch . no_grad (): batch = torch . stack ([ torch . tensor ( arr , device = self . device ) for arr in data ]) output = self . model ( batch ) top1 = torch . argmax ( output , dim = 1 ) return top1 . cpu () . tolist () class Postprocess ( Worker ): def __init__ ( self ): super () . __init__ () logger . info ( \"loading categories file...\" ) local_filename , _ = urlretrieve ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" ) with open ( local_filename ) as f : self . categories = list ( map ( lambda x : x . strip (), f . readlines ())) def forward ( self , data : int ) -> CategoryResp : return CategoryResp ( category = self . categories [ data ]) if __name__ == \"__main__\" : server = Server ( ImageReq , CategoryResp ) server . append_worker ( Preprocess , num = 4 ) server . append_worker ( Inference , max_batch_size = INFERENCE_BATCH_SIZE ) server . append_worker ( Postprocess , num = 1 ) server . run () Object Detection \u00a4 Image Segmentation \u00a4","title":"PyTorch"},{"location":"example/pytorch/#computer-vision","text":"Computer vision model servers usually receive images or links to the images (downloading from the link becomes an I/O workload then), feed the preprocessed image data into the model and extract information from the output results.","title":"Computer Vision"},{"location":"example/pytorch/#image-recognition","text":"This server receives an image and classify it according to the ImageNet categorization. We specifically use ResNet as an image classifier and build a model service based on it. Nevertheless, this file serves as the starter code for any kind of image recognition model server. We enable multiprocessing for Preprocess stage, so that it can produce enough tasks for Inference stage to do batch inference , which better exploits the GPU computing power. resnet50.py import base64 import logging from typing import List from urllib.request import urlretrieve import cv2 # type: ignore import numpy as np # type: ignore import torch # type: ignore import torchvision # type: ignore from pydantic import BaseModel from mosec import Server , Worker logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 16 class ImageReq ( BaseModel ): image : str # base64 encoded class CategoryResp ( BaseModel ): category : str # class name class Preprocess ( Worker ): def forward ( self , req : ImageReq ) -> np . ndarray : im = np . frombuffer ( base64 . b64decode ( req . image ), np . uint8 ) im = cv2 . imdecode ( im , cv2 . IMREAD_COLOR )[:, :, :: - 1 ] # bgr -> rgb im = cv2 . resize ( im , ( 256 , 256 )) crop_im = ( im [ 16 : 16 + 224 , 16 : 16 + 224 ] . astype ( np . float32 ) / 255 ) # center crop crop_im -= [ 0.485 , 0.456 , 0.406 ] crop_im /= [ 0.229 , 0.224 , 0.225 ] crop_im = np . transpose ( crop_im , ( 2 , 0 , 1 )) return crop_im class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = torchvision . models . resnet50 ( pretrained = True ) self . model . eval () self . model . to ( self . device ) # overwrite self.example for warmup self . example = [ np . zeros (( 3 , 244 , 244 ), dtype = np . float32 ) ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ np . ndarray ]) -> List [ int ]: logger . info ( f \"processing batch with size: { len ( data ) } \" ) with torch . no_grad (): batch = torch . stack ([ torch . tensor ( arr , device = self . device ) for arr in data ]) output = self . model ( batch ) top1 = torch . argmax ( output , dim = 1 ) return top1 . cpu () . tolist () class Postprocess ( Worker ): def __init__ ( self ): super () . __init__ () logger . info ( \"loading categories file...\" ) local_filename , _ = urlretrieve ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" ) with open ( local_filename ) as f : self . categories = list ( map ( lambda x : x . strip (), f . readlines ())) def forward ( self , data : int ) -> CategoryResp : return CategoryResp ( category = self . categories [ data ]) if __name__ == \"__main__\" : server = Server ( ImageReq , CategoryResp ) server . append_worker ( Preprocess , num = 4 ) server . append_worker ( Inference , max_batch_size = INFERENCE_BATCH_SIZE ) server . append_worker ( Postprocess , num = 1 ) server . run ()","title":"Image Recognition"},{"location":"example/pytorch/#object-detection","text":"","title":"Object Detection"},{"location":"example/pytorch/#image-segmentation","text":"","title":"Image Segmentation"},{"location":"feature/","text":"We explain the features with more details in this section.","title":"Overview"}]}